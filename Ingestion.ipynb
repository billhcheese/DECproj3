{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ccebebc-c0cb-4ef5-bc7d-262e38d7b78d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25a892f5-a563-4809-8058-b9ef204b00a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c00e09-61df-4d7e-97c0-f8a4226249ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Injest CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff10b715-4513-4708-9082-c7568149a65f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs_vals = dbutils.fs.ls(\"/Volumes/learningpipeline/bronze/raw_data_values\")\n",
    "for csv in fs_vals:\n",
    "    df = spark.read.format(\"csv\").load(csv.path, header=\"true\")\n",
    "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").saveAsTable(f\"learningpipeline.bronze.{csv.name[0:-4]}\")\n",
    "\n",
    "fs_manis = dbutils.fs.ls(\"/Volumes/learningpipeline/bronze/raw_data_manifest\")\n",
    "for csv in fs_manis:\n",
    "    df = spark.read.format(\"csv\").load(csv.path, header=\"true\")\n",
    "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").learningpipeline.bronze.changesince2010saveAsTable(f\"learningpipeline.bronze.{csv.name[0:-4]}\")\n",
    "\n",
    "#df = spark.read.format(\"csv\").load(\"/Volumes/learningpipeline/bronze/raw_data_values/*.csv\")\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75aa78e8-576c-4b9b-87e4-5b66b4577f05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Injust geodatabase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe3fe9e-c78a-4f47-ba2d-e4b0c67a4ac3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the path to the geodatabase in DBFS\n",
    "gdb_path = \"/Workspace/Users/billhcheesey@gmail.com/1.MaterGeo.gdb\"\n",
    "\n",
    "# Load a specific layer from the geodatabase\n",
    "layer_name = \"ACS2022allgeo2022\"\n",
    "gdf = gpd.read_file(gdb_path, layer=layer_name)\n",
    "\n",
    "wkt = gdf.to_wkt()\n",
    "df = pd.DataFrame(wkt)\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").saveAsTable(f\"learningpipeline.bronze.{layer_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
